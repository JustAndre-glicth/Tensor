{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tensor/onthotedcoder.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjAmwQzw_upd"
      },
      "source": [
        ""
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gjPZC2r5M1R",
        "outputId": "f31d1173-14e3-49c3-e7f5-d65df1935078"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGcwqZ0-5jsm"
      },
      "source": [
        "# Imports\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import seaborn as sns\r\n",
        "from sklearn.preprocessing import StandardScaler\r\n",
        "from sklearn.cluster import KMeans\r\n",
        "from sklearn.cluster import AgglomerativeClustering\r\n",
        "import scipy.cluster.hierarchy as sch\r\n",
        "plt.style.use('fivethirtyeight')"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "B8lHxyTY5m6u",
        "outputId": "f4b60307-8d9b-4154-df22-46f747cef3a3"
      },
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/PTDataScience/PandasForDataManipulation/breast_cancer.csv')\r\n",
        "df.head()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>diagnosis</th>\n",
              "      <th>radius_mean</th>\n",
              "      <th>texture_mean</th>\n",
              "      <th>perimeter_mean</th>\n",
              "      <th>area_mean</th>\n",
              "      <th>smoothness_mean</th>\n",
              "      <th>compactness_mean</th>\n",
              "      <th>concavity_mean</th>\n",
              "      <th>concave points_mean</th>\n",
              "      <th>symmetry_mean</th>\n",
              "      <th>fractal_dimension_mean</th>\n",
              "      <th>radius_se</th>\n",
              "      <th>texture_se</th>\n",
              "      <th>perimeter_se</th>\n",
              "      <th>area_se</th>\n",
              "      <th>smoothness_se</th>\n",
              "      <th>compactness_se</th>\n",
              "      <th>concavity_se</th>\n",
              "      <th>concave points_se</th>\n",
              "      <th>symmetry_se</th>\n",
              "      <th>fractal_dimension_se</th>\n",
              "      <th>radius_worst</th>\n",
              "      <th>texture_worst</th>\n",
              "      <th>perimeter_worst</th>\n",
              "      <th>area_worst</th>\n",
              "      <th>smoothness_worst</th>\n",
              "      <th>compactness_worst</th>\n",
              "      <th>concavity_worst</th>\n",
              "      <th>concave points_worst</th>\n",
              "      <th>symmetry_worst</th>\n",
              "      <th>fractal_dimension_worst</th>\n",
              "      <th>Unnamed: 32</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>842302</td>\n",
              "      <td>M</td>\n",
              "      <td>17.99</td>\n",
              "      <td>10.38</td>\n",
              "      <td>122.80</td>\n",
              "      <td>1001.0</td>\n",
              "      <td>0.11840</td>\n",
              "      <td>0.27760</td>\n",
              "      <td>0.3001</td>\n",
              "      <td>0.14710</td>\n",
              "      <td>0.2419</td>\n",
              "      <td>0.07871</td>\n",
              "      <td>1.0950</td>\n",
              "      <td>0.9053</td>\n",
              "      <td>8.589</td>\n",
              "      <td>153.40</td>\n",
              "      <td>0.006399</td>\n",
              "      <td>0.04904</td>\n",
              "      <td>0.05373</td>\n",
              "      <td>0.01587</td>\n",
              "      <td>0.03003</td>\n",
              "      <td>0.006193</td>\n",
              "      <td>25.38</td>\n",
              "      <td>17.33</td>\n",
              "      <td>184.60</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>0.1622</td>\n",
              "      <td>0.6656</td>\n",
              "      <td>0.7119</td>\n",
              "      <td>0.2654</td>\n",
              "      <td>0.4601</td>\n",
              "      <td>0.11890</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>842517</td>\n",
              "      <td>M</td>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0.07864</td>\n",
              "      <td>0.0869</td>\n",
              "      <td>0.07017</td>\n",
              "      <td>0.1812</td>\n",
              "      <td>0.05667</td>\n",
              "      <td>0.5435</td>\n",
              "      <td>0.7339</td>\n",
              "      <td>3.398</td>\n",
              "      <td>74.08</td>\n",
              "      <td>0.005225</td>\n",
              "      <td>0.01308</td>\n",
              "      <td>0.01860</td>\n",
              "      <td>0.01340</td>\n",
              "      <td>0.01389</td>\n",
              "      <td>0.003532</td>\n",
              "      <td>24.99</td>\n",
              "      <td>23.41</td>\n",
              "      <td>158.80</td>\n",
              "      <td>1956.0</td>\n",
              "      <td>0.1238</td>\n",
              "      <td>0.1866</td>\n",
              "      <td>0.2416</td>\n",
              "      <td>0.1860</td>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.08902</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>84300903</td>\n",
              "      <td>M</td>\n",
              "      <td>19.69</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "      <td>0.15990</td>\n",
              "      <td>0.1974</td>\n",
              "      <td>0.12790</td>\n",
              "      <td>0.2069</td>\n",
              "      <td>0.05999</td>\n",
              "      <td>0.7456</td>\n",
              "      <td>0.7869</td>\n",
              "      <td>4.585</td>\n",
              "      <td>94.03</td>\n",
              "      <td>0.006150</td>\n",
              "      <td>0.04006</td>\n",
              "      <td>0.03832</td>\n",
              "      <td>0.02058</td>\n",
              "      <td>0.02250</td>\n",
              "      <td>0.004571</td>\n",
              "      <td>23.57</td>\n",
              "      <td>25.53</td>\n",
              "      <td>152.50</td>\n",
              "      <td>1709.0</td>\n",
              "      <td>0.1444</td>\n",
              "      <td>0.4245</td>\n",
              "      <td>0.4504</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.3613</td>\n",
              "      <td>0.08758</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>84348301</td>\n",
              "      <td>M</td>\n",
              "      <td>11.42</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0.28390</td>\n",
              "      <td>0.2414</td>\n",
              "      <td>0.10520</td>\n",
              "      <td>0.2597</td>\n",
              "      <td>0.09744</td>\n",
              "      <td>0.4956</td>\n",
              "      <td>1.1560</td>\n",
              "      <td>3.445</td>\n",
              "      <td>27.23</td>\n",
              "      <td>0.009110</td>\n",
              "      <td>0.07458</td>\n",
              "      <td>0.05661</td>\n",
              "      <td>0.01867</td>\n",
              "      <td>0.05963</td>\n",
              "      <td>0.009208</td>\n",
              "      <td>14.91</td>\n",
              "      <td>26.50</td>\n",
              "      <td>98.87</td>\n",
              "      <td>567.7</td>\n",
              "      <td>0.2098</td>\n",
              "      <td>0.8663</td>\n",
              "      <td>0.6869</td>\n",
              "      <td>0.2575</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>0.17300</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>84358402</td>\n",
              "      <td>M</td>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0.13280</td>\n",
              "      <td>0.1980</td>\n",
              "      <td>0.10430</td>\n",
              "      <td>0.1809</td>\n",
              "      <td>0.05883</td>\n",
              "      <td>0.7572</td>\n",
              "      <td>0.7813</td>\n",
              "      <td>5.438</td>\n",
              "      <td>94.44</td>\n",
              "      <td>0.011490</td>\n",
              "      <td>0.02461</td>\n",
              "      <td>0.05688</td>\n",
              "      <td>0.01885</td>\n",
              "      <td>0.01756</td>\n",
              "      <td>0.005115</td>\n",
              "      <td>22.54</td>\n",
              "      <td>16.67</td>\n",
              "      <td>152.20</td>\n",
              "      <td>1575.0</td>\n",
              "      <td>0.1374</td>\n",
              "      <td>0.2050</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>0.1625</td>\n",
              "      <td>0.2364</td>\n",
              "      <td>0.07678</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         id diagnosis  ...  fractal_dimension_worst  Unnamed: 32\n",
              "0    842302         M  ...                  0.11890          NaN\n",
              "1    842517         M  ...                  0.08902          NaN\n",
              "2  84300903         M  ...                  0.08758          NaN\n",
              "3  84348301         M  ...                  0.17300          NaN\n",
              "4  84358402         M  ...                  0.07678          NaN\n",
              "\n",
              "[5 rows x 33 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsLPP5Jk-DmV"
      },
      "source": [
        "X = df.iloc[:, 3:31].values  #Note: Exclude Last column with all NaN values.\r\n",
        "y = df.iloc[:, 2].values"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "to3JRTPF-GWZ"
      },
      "source": [
        "#Encoding Categorical Data\r\n",
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "labelencoder = LabelEncoder()\r\n",
        "\r\n",
        "y = labelencoder.fit_transform(y)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIl_HNavBNuL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29a19f6c-f916-4aac-dfe3-03226ee4d8d5"
      },
      "source": [
        "y"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([370, 426, 406,  98, 418, 159, 376, 233, 192, 160, 324, 322, 394,\n",
              "       323, 234, 273, 280, 328, 410, 222, 196,  28, 310, 436, 341, 351,\n",
              "       274, 381, 308, 362, 382, 125, 347, 398, 328, 343, 259, 194, 295,\n",
              "       217, 213,  73, 391, 206, 201, 383,   4, 201, 137, 218, 122, 227,\n",
              "       131, 375, 301, 106, 397, 282, 195,   9,  49,   8, 259,  19, 170,\n",
              "       285,  27,  91,  17, 176, 388,  14, 353, 239, 151, 326, 221, 373,\n",
              "       415, 181, 100, 208, 451, 392, 134, 379, 270, 390, 154, 279, 278,\n",
              "       311, 205, 214, 299, 417, 142,  41, 109, 266, 225,   0, 142,  43,\n",
              "        58, 197, 113, 154, 443,  94,  40, 167, 260,  59,  11, 130,  15,\n",
              "       289, 322, 369,  97, 384, 449, 271, 209, 242, 225, 389, 301, 408,\n",
              "       143, 312, 330, 319, 378, 175, 119,  99, 292,  88,  37, 327,  99,\n",
              "       185,  65, 129, 123, 292, 267, 235, 192,   5,  36,  81, 199, 148,\n",
              "       364, 345, 138,  70, 121, 396, 404, 153, 446, 294,  66, 344, 360,\n",
              "       294, 152, 212, 312,  78,  63,  10,  44, 338, 193, 178, 453, 434,\n",
              "       318,  97, 307,  47, 377, 119, 124, 150, 258, 175,  35, 153, 288,\n",
              "       186, 237, 374, 395, 268, 147, 361, 447, 240, 161, 302,  43, 346,\n",
              "       197, 306, 427, 125, 455, 358, 256, 243, 128,  51, 409, 402, 228,\n",
              "       223,  50, 321, 205, 263,  56, 296, 166, 179, 348,  92,  84, 424,\n",
              "        29, 251, 445, 423, 258, 359, 227, 157,  90, 236, 399,  57, 202,\n",
              "       184,  62, 106, 433, 104, 407, 356, 401, 248, 403, 309, 317, 315,\n",
              "       419, 357, 355, 316, 352, 431,  61, 224, 182,  64, 262,  89, 441,\n",
              "        38, 368, 128,  93, 386, 224, 242, 393, 120, 399, 332, 184, 165,\n",
              "       131, 184,  86,  96, 265, 293, 188, 126, 172, 237,  71, 122, 260,\n",
              "        59, 402, 160, 412,  58, 101, 109, 202,  16, 219, 195, 118, 277,\n",
              "       174, 107,   7, 162, 142, 375,  18, 158,  52, 414, 181, 420, 144,\n",
              "       169, 255, 135, 335, 334, 325, 190,  84,  85, 150, 349, 191, 385,\n",
              "        46, 448, 266,  30,  77, 405, 119,  53, 138, 284, 102, 132, 114,\n",
              "       321, 452, 300,  80, 164, 195, 244,  13,  26, 163, 207, 174, 339,\n",
              "       211, 421, 416, 145, 440, 442, 337, 304, 437, 430, 231, 331,  60,\n",
              "       215, 229,  78,  87,  76, 137, 155, 206, 276, 145, 245,  87, 403,\n",
              "        53,  12, 313, 439, 140, 254, 220, 177,  77, 123, 367, 130, 189,\n",
              "       187, 153,  72, 329, 180, 370, 149,  95,  76,  23, 295, 303, 128,\n",
              "        24, 314, 171,  82, 108, 281, 110, 229,  38,  45,  57,  66,  79,\n",
              "       172, 290, 156, 415, 387, 288, 249, 182, 252, 242, 250,  75, 354,\n",
              "       238,  60, 372, 133, 365, 286, 272, 435, 127, 404, 134, 272, 166,\n",
              "       210, 112, 203, 192,  39, 350, 454, 264, 109, 201, 204, 198,  32,\n",
              "       363, 111,  31, 136, 291, 149,  69, 179, 257, 246, 103, 333, 141,\n",
              "       246, 216, 232, 320, 159, 279, 400, 116, 342, 148, 366, 371, 160,\n",
              "       200, 289, 168, 161, 380, 428, 297, 241, 163, 444,  20,  33, 146,\n",
              "        77, 336, 312, 120, 287, 211, 274, 298,  94, 377, 411, 183, 173,\n",
              "        21, 450,  86, 233,  42,   6, 215, 153, 247, 139, 121, 115, 230,\n",
              "       422,  74, 425, 261, 117,   2,   1, 107, 269, 283, 203, 244, 226,\n",
              "        55,  53,  34,  67,  68,  79, 175,  22, 183,  54,  48,  25, 275,\n",
              "       105, 253,  83, 305, 432, 438, 413, 340, 429,   3])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7ZE4i_r-J2t"
      },
      "source": [
        "#Splitting into Training set and Test set\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\r\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQ3Ca0KY-Mcg"
      },
      "source": [
        "# Importing the Keras libraries and packages\r\n",
        "import keras\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5B12B5TYB1wB"
      },
      "source": [
        "\r\n",
        "# define the keras model\r\n",
        "model = Sequential()\r\n",
        "model.add(Dense(12, input_dim=8, activation='relu'))\r\n",
        "model.add(Dense(8, activation='relu'))\r\n",
        "model.add(Dense(1, activation='sigmoid'))\r\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGZTSgQfCC5K"
      },
      "source": [
        "# compile the keras model\r\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Q2JT9TzCGyy"
      },
      "source": [
        "# fit the keras model on the dataset\r\n",
        "#model.fit(X, y, epochs=150, batch_size=10)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnRp__YYDIVu"
      },
      "source": [
        "Ten"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oiIORAwrDHpM"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvoDDkJMDNJK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c0f4041-8b1d-472c-c937-e002235af853"
      },
      "source": [
        "mnist = tf.keras.datasets.mnist\r\n",
        "\r\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OoCMZI-mDSYm"
      },
      "source": [
        "model = tf.keras.models.Sequential([\r\n",
        "  tf.keras.layers.Flatten(input_shape=(28, 28)),\r\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\r\n",
        "  tf.keras.layers.Dropout(0.2),\r\n",
        "  tf.keras.layers.Dense(10)\r\n",
        "])"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4DO2S1DDTq4",
        "outputId": "a250a514-c35e-4443-c149-5b18abf65c29"
      },
      "source": [
        "predictions = model(x_train[:1]).numpy()\r\n",
        "predictions"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.07308622, -0.15532127,  0.11821122, -0.07689913,  0.11125173,\n",
              "        -0.08314126,  0.19864693, -1.3513356 ,  0.67411345, -0.23767044]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qncUodR4ED47",
        "outputId": "d8410e50-9e83-4c2e-897c-e75ecf0ccd99"
      },
      "source": [
        "tf.nn.softmax(predictions).numpy()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.10495093, 0.08351994, 0.10979532, 0.09033343, 0.10903385,\n",
              "        0.08977132, 0.1189917 , 0.02525619, 0.19142962, 0.07691772]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mmjMX08EH5O"
      },
      "source": [
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "crqg0rxkEJf1",
        "outputId": "505a3892-097c-439e-ca5c-5a9b28511a0d"
      },
      "source": [
        "loss_fn(y_train[:1], predictions).numpy()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.4104898"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obBwYFiyENm6"
      },
      "source": [
        "model.compile(optimizer='adam',\r\n",
        "              loss=loss_fn,\r\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K4kHbQhNEPei",
        "outputId": "045dda0c-9a0f-4c4e-a2ba-5342dfa4bb82"
      },
      "source": [
        "model.fit(x_train, y_train, epochs=5)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.4894 - accuracy: 0.8585\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1510 - accuracy: 0.9550\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1094 - accuracy: 0.9673\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0882 - accuracy: 0.9724\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0791 - accuracy: 0.9752\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f996f8c5c90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rS3t7q-EESb0",
        "outputId": "6ed9b85e-393a-41f5-bc54-36e05f2b6f93"
      },
      "source": [
        "model.evaluate(x_test,  y_test, verbose=2)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 - 0s - loss: 0.0775 - accuracy: 0.9767\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.07746010273694992, 0.9767000079154968]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yp3GIa12EYi4"
      },
      "source": [
        "probability_model = tf.keras.Sequential([\r\n",
        "  model,\r\n",
        "  tf.keras.layers.Softmax()\r\n",
        "])"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cGGNuvcfEaPP",
        "outputId": "61a76413-af15-435a-d12e-522a79153ca0"
      },
      "source": [
        "probability_model(x_test[:5])"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(5, 10), dtype=float32, numpy=\n",
              "array([[9.3517452e-09, 5.3831639e-10, 1.9381729e-07, 2.0128666e-05,\n",
              "        7.2944336e-13, 2.8443390e-07, 4.2520411e-16, 9.9997365e-01,\n",
              "        1.4471640e-07, 5.5777605e-06],\n",
              "       [2.3176641e-07, 2.6455596e-03, 9.9714547e-01, 1.6410918e-04,\n",
              "        2.6492588e-15, 1.0934681e-05, 1.6405931e-09, 7.2205618e-12,\n",
              "        3.3618195e-05, 3.8590099e-12],\n",
              "       [8.3696975e-08, 9.9947506e-01, 8.6080458e-05, 6.4978954e-06,\n",
              "        5.3772306e-05, 2.7584861e-06, 1.5189113e-06, 2.8791180e-04,\n",
              "        8.4511048e-05, 1.8985050e-06],\n",
              "       [9.9897230e-01, 7.8885972e-07, 4.4507949e-04, 2.7924066e-06,\n",
              "        9.8222949e-07, 3.9414846e-04, 2.4422197e-05, 1.9342664e-05,\n",
              "        1.8138921e-06, 1.3835906e-04],\n",
              "       [3.1962982e-06, 2.3110214e-08, 2.4710587e-05, 4.0568608e-07,\n",
              "        9.9452329e-01, 2.3863377e-06, 1.9121094e-06, 3.1316031e-05,\n",
              "        7.9641213e-06, 5.4046977e-03]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cvRMn9Y7Eciy",
        "outputId": "93f93491-2aa1-480b-fef1-2431739ea32a"
      },
      "source": [
        "# evaluate the keras model\r\n",
        "accuracy = model.evaluate(x_test, y_test)\r\n",
        "print(accuracy*100)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 0s 1ms/step - loss: 0.0775 - accuracy: 0.9767\n",
            "[0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968, 0.07746010273694992, 0.9767000079154968]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVXcHvDYFbWs",
        "outputId": "231c8bd2-9015-493e-e384-290cd0dfab29"
      },
      "source": [
        "accuracy = model.evaluate(x_test, y_test, verbose=0)\r\n",
        "accuracy*100000"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " 0.07746010273694992,\n",
              " 0.9767000079154968,\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    }
  ]
}